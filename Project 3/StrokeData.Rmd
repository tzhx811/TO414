---
title: "Predicting Stroke"
author: "Evan Jiang, Anthony Zhao, Jenny Yu, Xincheng Yuan, Yusuf Uzhunnan"
date: "4/25/2021"
output: 
  html_document:
      toc: true
      toc_float: true
      theme: readable
      highlight: tango
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
In this project, we are attempting to accurately predict whether a patient will have a stroke or not based on input variables such as age, gender, BMI, hypertension, glucose level, etc. We were originally attracted to this data set because it involves discovering practical insights that might ultimately lead to saving someone's life. A stroke is a scary event for someone to go through, so by creating predictive models, we hope to get a better insight on not only which variables have the strongest effect on a patient's probability of having a stroke, but also which variables are most significant to target when trying to prevent a risk of stroke. While somewhat rare, the effects of having a stroke can be life-changing. We believe that there are two main business applications to this project. The first is the most obvious connection: saving lives. Whether it be a family medicine doctor or an emergency room respondent, knowing how certain variables interact with the chance of stroke can lead to better prescriptions, better service, and more lives saved. The second business application is less obvious, but still quite relevant: insurance agencies. Insurance agencies can take the information of the patient, input it into the models created, and see the chance of stroke that the client may have. This can go into their modeling of medical insurance packages as well as the the placement of the client within the packages' different level of coverage and risk premium.

Next, we will start by loading and cleaning the data so it is prepared for analysis. From there, we will be creating different graphs and visuals to see if certain input variables interact with each other or have a stronger interaction force with the response variable (stroke or not). If any strong correlations are discovered from the data visualization section, we will take note and implement different train and test splits on top of our general models. After data visualization, we start to create the general models: SVM, Decision Tree, KNN, Logistic Regression, and ANN. Within each of these model chunks, we will create a final model that involves all the variables, but broken into a normalized train and test split, and a confusion matrix that compares the model's prediction to the test data. From there, we will also run more models and confusion matrices on different categorical splits. Some examples may be: married versus non-married, hypertension versus non-hypertension, whatever is discovered from the beginnings of our researching in data visualization. Ultimately, we will end with a stacked model that combines the predictions from our main general models and makes a prediction from those by assigning weights to the individual models' predictions. Once the stacked model is made, we will improve it using the train() command learned in class. This will be paired with a confusion matrix of which we can compare performance statistics between the individual models and the stacked model. We will end with a summary of our findings. 

## Data Preparation

### Library Loading
```{r, warning=FALSE}
library(C50)
library(class)
library(gmodels)
library(caret)
library(lmtest)
library(aod)
library(VGAM)
library(neuralnet)
library(kernlab)
library(lsr)
```

### Load in Data
```{r}
stroke <- read.csv("healthcare-dataset-stroke-data.csv")
stroke <- stroke[stroke$gender != "Other",]
stroke$id <- NULL
stroke$gender <- as.factor(stroke$gender)
stroke$work_type <- as.factor(stroke$work_type)
stroke$smoking_status <- as.factor(stroke$smoking_status)
stroke$ever_married <- as.factor(stroke$ever_married)
stroke$Residence_type <- as.factor(stroke$Residence_type)
summary(stroke)
```

We removed the one instance of gender being neither female nor male in this dataset, as one observation would not lead to a statistically significant result. We also removed the ID column, as that would bear no use to predicting strokes.

### Create Linear Model to Predict Missing BMI
```{r}
# create the bmi data that doesn't have NA
bmi_data <- stroke[stroke$bmi != "N/A",]
# can't use stroke to predict bmi
bmi_model <- lm(bmi ~ .-stroke, data = bmi_data)
summary(bmi_model)
# remove insignificant variables
bmi_model <- lm(bmi ~ .-stroke-gender-Residence_type+age*hypertension*ever_married-heart_disease, data = bmi_data)
summary(bmi_model)

stroke$bmi[stroke$bmi == "N/A"] = predict(bmi_model, stroke[stroke$bmi == "N/A",])
stroke$bmi <- as.numeric(stroke$bmi)
```

A lot of values in the BMI column was left N/A, so to counter this issue, we made a linear model to try and predict BMI based on the other variables except stroke. The R squared value was less than 0.27, so the model doesn't actually do a great job explaining variance in data. However, to keep more data usable in the later predictions, we decided to use the BMI calculations.

### Define Helper Functions
```{r}
set.seed(42)

normalize <- function(x) {
  if(max(x) - min(x) == 0){
    return (x)
  }
  return ((x - min(x)) / (max(x) - min(x)))
}

set_splits <- function(x, norm) {
  if(norm){
    x = as.data.frame(lapply(as.data.frame(model.matrix(~.-1,x)), normalize))
  }
  testrows <- sample(1:nrow(x), 0.25*nrow(x))
  return (list(x[-testrows,], x[testrows,]))
}
```

We decided to make some helper functions to normalize the data and create splits.

## Visualizations

### Initial Visualizations
```{r}
par(mfrow=c(2,2))
barplot(prop.table(table(stroke$stroke, stroke$gender), 2), main="Gender vs Stroke", xlab="Gender", col=c("darkblue","red"))
barplot(prop.table(table(stroke$stroke, stroke$hypertension), 2), main="Hypertension vs Stroke", xlab="Hypertension", col=c("darkblue","red"))
barplot(prop.table(table(stroke$stroke, stroke$heart_disease), 2), main="Heart Disease vs Stroke", xlab="Heart Disease", col=c("darkblue","red"))
barplot(prop.table(table(stroke$stroke, stroke$ever_married), 2), main="Marriage vs Stroke", xlab="Marriage", col=c("darkblue","red"))
par(mfrow=c(2,2))
barplot(prop.table(table(stroke$stroke, stroke$work_type), 2), main="Work vs Stroke", xlab="Work", col=c("darkblue","red"))
barplot(prop.table(table(stroke$stroke, stroke$Residence_type), 2), main="Residence vs Stroke", xlab="Residence Type", col=c("darkblue","red"))
barplot(prop.table(table(stroke$stroke, stroke$smoking_status), 2), main="Smoker vs Stroke", xlab="Smoking Status", col=c("darkblue","red"))
barplot(prop.table(table(stroke$stroke, quantileCut(stroke$age, 10)), 2), main="Age vs Stroke", xlab="Age", col=c("darkblue","red"))
par(mfrow=c(2,2))
barplot(prop.table(table(stroke$stroke, quantileCut(stroke$bmi, 10)), 2), main="BMI vs Stroke", xlab="BMI", col=c("darkblue","red"))
barplot(prop.table(table(stroke$stroke, quantileCut(stroke$avg_glucose_level, 10)), 2), main="Glucose vs Stroke", xlab="Glucose Level", col=c("darkblue","red"))
```

First, we wanted to visualize how the data related to the variable of interest: stroke or no stroke. Looking at the plots created, there is a higher proportion of people getting a stroke from those with heart disease, hypertension, and marriage. However, all these attributes are related to age, which also leads to higher proportions of strokes as it increases. Residence type doesn't change the proportion of strokes, and smoking and work status doesn't show a strong correlation. Glucose levels only has a large impact at the very extreme ends, but oddly enough, average levels of BMI led to higher proportions of stroke.

### Inter-Variable Visualization
```{r}
prop.table(table(stroke$ever_married, stroke$hypertension), 1)
prop.table(table(stroke$ever_married, stroke$heart_disease), 1)
prop.table(table(stroke$hypertension, stroke$heart_disease), 1)
par(mfrow=c(2,2))
barplot(prop.table(table(stroke$ever_married, quantileCut(stroke$age, 10)), 2), main="Age vs Marriage", xlab="Age", col=c("darkblue","red"))
barplot(prop.table(table(stroke$hypertension, quantileCut(stroke$age, 10)), 2), main="Age vs Hypertension", xlab="Age", col=c("darkblue","red"))
barplot(prop.table(table(stroke$heart_disease, quantileCut(stroke$age, 10)), 2), main="Age vs Heart Disease", xlab="Age", col=c("darkblue","red"))
par(mfrow=c(2,2))
barplot(prop.table(table(stroke$heart_disease[stroke$ever_married=="Yes"], cut(stroke$age[stroke$ever_married=="Yes"], breaks = c(0,10,20,30,40,50,60,70,80,90))), 2), main="Age vs Heart Disease for Married", xlab="Age", col=c("darkblue","red"))
barplot(prop.table(table(stroke$heart_disease[stroke$ever_married=="No"], cut(stroke$age[stroke$ever_married=="No"], breaks = c(0,10,20,30,40,50,60,70,80,90))), 2), main="Age vs Heart Disease for Unmarried", xlab="Age", col=c("darkblue","red"))
barplot(prop.table(table(stroke$hypertension[stroke$ever_married=="Yes"], cut(stroke$age[stroke$ever_married=="Yes"], breaks = c(0,10,20,30,40,50,60,70,80,90))), 2), main="Age vs Hypertension for Married", xlab="Age", col=c("darkblue","red"))
barplot(prop.table(table(stroke$hypertension[stroke$ever_married=="No"], cut(stroke$age[stroke$ever_married=="No"], breaks = c(0,10,20,30,40,50,60,70,80,90))), 2), main="Age vs Hypertension for Unmarried", xlab="Age", col=c("darkblue","red"))

```

First, looking at how marriage, hypertension, and heart disease relate to one another, it seems the effect is negligible. Both factors more strongly correlate to age. As shown in the first set of graphs, increasing age leads to much higher chances of marriage, heart disease, and hypertension. One of the factors that was suggested to correlate to hypertension and heart disease was marriage, and factoring in the effect of age, we can see in the second set of graphs that being married and unmarried did not significantly change the proportion of people who had a stroke then.

Thus, we decided to split based on BMI and Age in our models, since those two variables seem to have a large effect on probability of stroke. The BMI range of between 26.5 to 32 seemed to have a higher proportion of strokes, and being older than 55 also led to much higher chances, so having seperate models will hopefully improve accuracy overall.

## Create Splits
```{r}
general_norm <- set_splits(stroke, norm = TRUE)
age_young_data_norm = set_splits(stroke[stroke$age<=55,], norm = TRUE)
age_old_data_norm = set_splits(stroke[stroke$age>55,], norm = TRUE)
bmi_risk_data_norm = set_splits(stroke[stroke$bmi>26.5 & stroke$bmi<32,], norm = TRUE)
bmi_safe_data_norm = set_splits(stroke[stroke$bmi<=26.5 | stroke$bmi>=32,], norm = TRUE)
```

## SVM Model
We wanted to run a SVM model to see that if can accurately classify "stroke" or "no stroke" based on the variables given. SVM models are known to be quite accurate in classification scenarios. Because our data set is quite clean and somewhat small, we believe that an SVM data set can accurately predict "stroke" or "no stroke." It is also quite efficient given that it uses a subset of training points rather than all individual training points.

### General Model
```{r}
stroke_svm <- ksvm(stroke ~ ., data = general_norm[[1]], kernel = "rbfdot")
svm_pred <- predict(stroke_svm, general_norm[[2]])
cutoff = quantile(svm_pred, 1-mean(general_norm[[1]]$stroke))
svm_pred = ifelse(svm_pred>cutoff, 1, 0)
confusionMatrix(as.factor(svm_pred), as.factor(general_norm[[2]]$stroke))
```

The SVM model was used with RBF kernel and a cutoff according to the proportion of people in the train data who suffered a stroke. This led to a kappa statistic of 0.0969, very low and indicative of a poor model. Next we want to see if splitting the data based on age and BMI would increase the model's accuracy or kappa statistic. 

### SVM Age Based Model
```{r}
# first is young model
young_svm <- ksvm(stroke ~ ., data = age_young_data_norm[[1]], kernel = "rbfdot")
young_pred <- predict(young_svm, age_young_data_norm[[2]])
cutoff = quantile(young_pred, 1-mean(age_young_data_norm[[1]]$stroke))
young_pred = ifelse(young_pred>cutoff, 1, 0)
# now old model
old_svm <- ksvm(stroke ~ ., data = age_old_data_norm[[1]], kernel = "rbfdot")
old_pred <- predict(old_svm, age_old_data_norm[[2]])
cutoff = quantile(old_pred, 1-mean(age_old_data_norm[[1]]$stroke))
old_pred = ifelse(old_pred>cutoff, 1, 0)

# now combine them together
combined_svmage_pred = c(young_pred, old_pred)
# get the true labels
true_label = c(age_young_data_norm[[2]]$stroke, age_old_data_norm[[2]]$stroke)
confusionMatrix(as.factor(combined_svmage_pred), as.factor(true_label))
```

When we create two models based on age and combine their results, we can see that the kappa statistic increases to 0.1249, a lot higher than that of the general model.

### SVM BMI Based Model
```{r}
risk_svm <- ksvm(stroke ~ ., data = bmi_risk_data_norm[[1]], kernel = "rbfdot")
risk_pred <- predict(risk_svm, bmi_risk_data_norm[[2]])
cutoff = quantile(risk_pred, 1-mean(bmi_risk_data_norm[[1]]$stroke))
risk_pred = ifelse(risk_pred>cutoff, 1, 0)

safe_svm <- ksvm(stroke ~ ., data = bmi_safe_data_norm[[1]], kernel = "rbfdot")
safe_pred <- predict(safe_svm, bmi_safe_data_norm[[2]])
cutoff = quantile(safe_pred, 1-mean(bmi_safe_data_norm[[1]]$stroke))
safe_pred = ifelse(safe_pred>cutoff, 1, 0)

combined_svmbmipred = c(risk_pred, safe_pred)
true_label = c(bmi_risk_data_norm[[2]]$stroke, bmi_safe_data_norm[[2]]$stroke)
confusionMatrix(as.factor(combined_svmbmipred), as.factor(true_label))
```

The best kappa statistic achieved by the SVM is when splitting on BMI, with resulting kappa of 0.1963. It seems that normal BMI range has different predictors of stroke likelihood than the two tails, as does being older compared to younger. Overall though, the SVM was not a great classifier of strokes. With the use of RBF kernels, the coefficients also become un-interpretable, so SVM in this task was not particularly useful.

## Decision Tree
Next, we want to run a Decision Tree model to see if it would perform good predictions. The benefits of knn model include its managerial insights on which variables have the biggest impact on whether or not a person has a stroke.
```{r}
#rebalance errors - want to prioritize minimizing false negatives, if people are at risk of stroke we want to know, used 20 to make it about 20% of true positives show up as negatives. this is a number that we decided on as a group in which we would feel comfortable with if we were the doctor or hospital giving advice
error_cost <- matrix(c(0, 1, 20, 0), nrow = 2)

#build decision tree
stroke_dt <- C5.0(as.factor(stroke) ~ ., data = general_norm[[1]], costs = error_cost)

plot(stroke_dt)
summary(stroke_dt)

#predict the test data
stroke_dt_pred <- predict(stroke_dt, general_norm[[2]])
CrossTable(general_norm[[2]]$stroke, stroke_dt_pred)

# Confusion Matrix and Kappa Statistics


confusionMatrix(stroke_dt_pred, as.factor(general_norm[[2]]$stroke))
```

Our general Decision Tree model  has a kappa value of 0.1495, meaning that using this model by itself would predict stokes 14.95% better than if it was just random. The accuracy of the model is 72.44% overall, and out of 66 patients who do have stroke, our model predicted 51 of them correctly (77.27% accuracy). 
Next we want to see if splitting the data based on age and BMI would increase the model's accuracy or kappa statistic for our Decision Tree model.

### Decision Trees Age Based Model
```{r}

summary(age_old_data_norm[[1]])
#Young Patients
ageYoung_dt <- C5.0(as.factor(stroke) ~ ., data = age_young_data_norm[[1]], costs = error_cost)
plot(ageYoung_dt)
summary(ageYoung_dt)

#predict the test data
ageYoung_dt_pred <- predict(ageYoung_dt, age_young_data_norm[[2]])


#Old Patients
ageOld_dt <- C5.0(as.factor(stroke) ~ ., data = age_old_data_norm[[1]], costs = error_cost)
plot(ageOld_dt)
summary(ageOld_dt)

#predict the test data
ageOld_dt_pred <- predict(ageOld_dt, age_old_data_norm[[2]])


#Combined confusion matrix
combinedagepredict <- c(ageOld_dt_pred,ageYoung_dt_pred)-1
summary(combinedagepredict)

combinedageactual <- c(age_old_data_norm[[2]]$stroke, age_young_data_norm[[2]]$stroke)
summary(combinedageactual)

confusionMatrix(as.factor(combinedagepredict), as.factor(combinedageactual))
```

When we create a Decision Tree model based on an age split, we can see that accuracy increases a little bit while the kappa statistic decreases a little bit. This indicates about the same performance as the general model. However, it does show that those who are younger have different predictor variables for stroke when compared to those that are older. For example, For the younger group, age, hypertension, and average glucose levels were the significant variables in predicting stroke while for the older group, age, bmi, and average glucose level were the significant variables. This could mean that as a patient gets older, their BMI is a greater indicator of other potential health problems that may also affect stroke levels, or that it becomes a more prominent indicator of overall health and stroke chances.

### Decision Trees BMI Based Model
```{r}
# separate data for further insight

#Risky BMI Patients

bmiRisk_dt <- C5.0(as.factor(stroke) ~ ., data = bmi_risk_data_norm[[1]], costs = error_cost)
plot(bmiRisk_dt)
summary(bmiRisk_dt)

#predict the test data
bmiRisk_dt_pred <- predict(bmiRisk_dt, bmi_risk_data_norm[[2]])


#Not Risky BMI Patients

bmiNotRisk_dt <- C5.0(as.factor(stroke) ~ ., data = bmi_safe_data_norm[[1]], costs = error_cost)
plot(bmiNotRisk_dt)
summary(bmiNotRisk_dt)

#predict the test data
bmiNotRisk_dt_pred <- predict(bmiNotRisk_dt, bmi_safe_data_norm[[2]])

#Combined confusion matrix
combinedbmipredict <- c(bmiNotRisk_dt_pred,bmiRisk_dt_pred)-1
summary(combinedbmipredict)

combinedbmiactual <- c(bmi_safe_data_norm[[2]]$stroke, bmi_risk_data_norm[[2]]$stroke)
summary(combinedbmiactual)

confusionMatrix(as.factor(combinedbmipredict), as.factor(combinedbmiactual))
```

When we create a Decision Tree model based on bmi split, we can see that accuracy increases a little bit while the kappa statistic increases by a more significant amount. This indicates that the bmi split performs a little bit better than the general model. In addition, it does show that those who have risky bmi levels have different predictor variables for stroke when compared to those that have not as risky bmi levels. For example, for those that have risky bmi levels, age, average glucose level, bmi, and hypertension were the significant variables in predicting stroke while for the non risky bmi group, age, heart disease, and smoking status were the significant variables. Overall, the confusion matrix output leads us to believe that predicting stroke based on different BMI categories is more effective than age and the general model. The thresholds for the BMI splits were created in similar fashion to the age splits where they were set to optimize kappa but ultimately led to predicting all "stroke" or "no stroke" depending on the BMI category.

## KNN Model
Next, we want to run a knn model use max-min normalization to see if it would perform good predictions. The benefits of knn model include its accuracy and fast execution time in predicting whether a person will have a stroke or not.
```{r}
set.seed(12345)
stroketrain_knn <- general_norm[[1]]
stroketrain_knn$stroke = NULL
stroketest_knn <- general_norm[[2]]
stroketest_knn$stroke = NULL

knn_train_labels <- general_norm[[1]]$stroke
knn_test_labels <- general_norm[[2]]$stroke

stroke_knn_test_pred <- knn(train = stroketrain_knn, test = stroketest_knn,
                      cl = knn_train_labels, k= 4)

confusionMatrix(as.factor(stroke_knn_test_pred), as.factor(knn_test_labels))
```

Our general KNN model using max-min normalization has a kappa value of 0.0768, with an accuracy of 0.9413. The value of k used was very small of 4, since larger numbers led to the effect of the nearest neighbors being diminished and always predicting 0 (no stroke), since the majority of the data is no stroke.
Next we want to see if splitting the data based on age and BMI would increase the model's accuracy or kappa statistic for our KNN model.

### KNN Model Age Based Model
```{r}
###Young patients
stroketrain_knn_young <- age_young_data_norm[[1]]
stroketrain_knn_young$stroke = NULL
stroketest_knn_young <- age_young_data_norm[[2]]
stroketest_knn_young$stroke = NULL
knn_train_labels_young <- age_young_data_norm[[1]]$stroke
knn_test_labels_young <- age_young_data_norm[[2]]$stroke

stroke_knn_pred_young <- knn(train = stroketrain_knn_young, test = stroketest_knn_young,
                      cl = knn_train_labels_young, k= 4)
confusionMatrix(as.factor(stroke_knn_pred_young), as.factor(knn_test_labels_young))

###Old patients
stroketrain_knn_old <- age_old_data_norm[[1]]
stroketrain_knn_old$stroke = NULL
stroketest_knn_old <- age_old_data_norm[[2]]
stroketest_knn_old$stroke = NULL
knn_train_labels_old <- age_old_data_norm[[1]]$stroke
knn_test_labels_old <- age_old_data_norm[[2]]$stroke

stroke_knn_pred_old <- knn(train = stroketrain_knn_old, test = stroketest_knn_old,
                      cl = knn_train_labels_old, k = 4)
confusionMatrix(as.factor(stroke_knn_pred_old), as.factor(knn_test_labels_old))

##Combine the results together
knn_age_combined_pred = c(stroke_knn_pred_young, stroke_knn_pred_old) - 1
true_label_age = c(age_young_data_norm[[2]]$stroke, age_old_data_norm[[2]]$stroke)
confusionMatrix(as.factor(knn_age_combined_pred), as.factor(true_label_age))
```

After splitting the data by age, we build 2 separate models and put them together for the combined model. As seen in the confusion matrices, the kappa value for the young-patients knn model is 0, while it is 0.034 for old-patients data. This is because there are only 8 young patients in our test data actually have stroke, while 61 old patients have stroke. As result, the KNN model is very powerful when predicting whether an old patient would have stroke.
The combined model results have a kappa value of 0.0692, which is slightly lower than the original model without the age split, likely since younger users are very inaccurate to precit strokes on due to the low number of observations.

### KNN Model BMI Based Model
```{r}
###Risky patients
stroketrain_knn_risk <- bmi_risk_data_norm[[1]]
stroketrain_knn_risk$stroke = NULL
stroketest_knn_risk <- bmi_risk_data_norm[[2]]
stroketest_knn_risk$stroke = NULL
knn_train_labels_risk <- bmi_risk_data_norm[[1]]$stroke
knn_test_labels_risk <- bmi_risk_data_norm[[2]]$stroke

stroke_knn_pred_risk <- knn(train = stroketrain_knn_risk, test = stroketest_knn_risk,
                      cl = knn_train_labels_risk, k= 4)
confusionMatrix(as.factor(stroke_knn_pred_risk), as.factor(knn_test_labels_risk))

###Safe patients
stroketrain_knn_safe <- bmi_safe_data_norm[[1]]
stroketrain_knn_safe$stroke = NULL
stroketest_knn_safe <- bmi_safe_data_norm[[2]]
stroketest_knn_safe$stroke = NULL
knn_train_labels_safe <- bmi_safe_data_norm[[1]]$stroke
knn_test_labels_safe <- bmi_safe_data_norm[[2]]$stroke

stroke_knn_pred_safe <- knn(train = stroketrain_knn_safe, test = stroketest_knn_safe,
                      cl = knn_train_labels_safe, k = 4)
confusionMatrix(as.factor(stroke_knn_pred_safe), as.factor(knn_test_labels_safe))

##Combined
knn_bmi_combined_pred = c(stroke_knn_pred_risk, stroke_knn_pred_safe) - 1
true_label_bmi = c(bmi_risk_data_norm[[2]]$stroke, bmi_safe_data_norm[[2]]$stroke)
confusionMatrix(as.factor(knn_bmi_combined_pred), as.factor(true_label_bmi))
```

After splitting the data by bmi value, we build 2 separate models and put them together for the combined model. As seen in the confusion matrices, the kappa value for the risky group (bmi value over 26.5 but under 32) knn model is only 0.0564, and it is 0.0513 for safe group. As result, our KNN model is more powerful when predicting results for patients with extreme bmi value.
When we combine two models' results together, the combined model results have a kappa value of 0.061.

Moving forward, the KNN models have pretty low kappa values and may not be considered as an important factor in the final stacked model. Since the dataset is small and strokes are a much smaller fraction of the observations, knn does not lend well to being a good model for this case.

## Logistic Regression Model
We wanted to run a logistic regression model to see if it would perform better than the other models. A benefit of a logistic regression model is that it will emphasis which variables are significant (and in what magnitude) in predicting whether a person will have a stroke or not. 
```{r}
#Mass Effect
MassEffect <- glm(stroke ~ ., data = general_norm[[1]], family = "binomial")
summary(MassEffect)

#Final
logit_final <- step(MassEffect)
summary(logit_final)

#Confusion Matrix (.11 gave the highest kappa statistic)
LogReg_Pred = predict(logit_final, newdata = general_norm[[2]], type = "response")
LogReg_Pred = ifelse(LogReg_Pred>.11, 1, 0)
confusionMatrix(data = as.factor(LogReg_Pred), reference = as.factor(general_norm[[2]]$stroke))
```

Based on the general model ran, we can see that out of all the original variables, there are significantly less that play a significant role in predicting stroke. We can see that age, hypertension, heart disease, and average glucose level are the most significant. We also see that all of the previously mentioned factors actually increase the log-odds ratio if having a stroke. With the final model, we can see that it is actually quite accurate in predicting stroke or not; it had an accuracy level of .8473. However, it only had a kappa statistic of .1843. This is mostly because there were a much larger number of "negative"(s) than "positive"(s) of stroke in the test data, therefore making the chance of simply guessing randomly correctly much higher if one was to guess "negative" on all the patients. While the model was quite accurate we would want to decrease the number of false negatives as much as possible because the last thing we want is to tell a patient that they are not at risk of having a stroke, and then they have a stroke because we didn't start the remedial process due to the model's prediction. To combat the model from simply either predicting all "negative" or all "positive", we changed the threshold from .5 to .11, being the cutoff point to when we would want to start warning patients of their potential to having a stroke. This is similar to instead of warning them when they have a 50% chance, we warn them when they have an 11% chance of stroke. 11% was the cutoff point that granted the highest kappa statistic. Next, we wanted to see if the age and BMI split could produce a better model. 

### Logistic Regression Age Based Model
```{r}
#Young
masslogmodel_young <- glm(stroke ~., data = age_young_data_norm[[1]], family = "binomial")
summary(masslogmodel_young)

finallogmodel_young <- step(masslogmodel_young)
summary(finallogmodel_young)

young_Logpred <- predict(finallogmodel_young, age_young_data_norm[[2]], type= "response")
young_Logpred = ifelse(young_Logpred>.0002, 0, 1)

#Old
masslogmodel_old <- glm(stroke ~., data = age_old_data_norm[[1]], family = "binomial")
summary(masslogmodel_old)

finallogmodel_old <- step(masslogmodel_old)
summary(finallogmodel_old)

old_Logpred <- predict(finallogmodel_old, age_old_data_norm[[2]], type= "response")
old_Logpred = ifelse(old_Logpred>.45, 0, 1)

#Combine Predictions (The thresholds above gave us the highest kappa statistic)
age_combined_Logpred = c(young_Logpred, old_Logpred)

age_true_label = c(age_young_data_norm[[2]]$stroke, age_old_data_norm[[2]]$stroke)

confusionMatrix(as.factor(age_combined_Logpred), as.factor(age_true_label))
```

When we create a logistic regression model based on an age split, we can see that accuracy decreases a fair bit while the kappa statistic also decreases somewhat. This indicates a weaker model than the general model. However, it does show that those who are younger have different predictor variables for stroke when compared to those that are older. For example, For the younger group, hypertension and age were the significant variables in predicting stroke while for the older group, age and average glucose level were the significant variables. While this does provide us with further insight, it overall generates a weaker model. This could be due to the over-categorization of age into "old" and "young". It is also important to note the different cutoff thresholds that we implemented for the "young model" and the "old model". For the "young model", we set the threshold to be: any prediction greater than .0002, predict stroke, otherwise, predict no stroke. For the "old model" we set the threshold to be .45. While setting the thresholds to this led to the "young model" predicting all "no stroke" and the "old model" predicting all "stroke," we chose these thresholds because they maximized the kappa statistic without sacrificing too much accuracy. This pattern can be explained by how in general, age is more significant when predicting stroke or no stroke. 

### Logistic Regression BMI Based Models
```{r}
#Risk
masslogmodel_risk <- glm(stroke ~., data = bmi_risk_data_norm[[1]], family = "binomial")
summary(masslogmodel_risk)

finallogmodel_risk <- step(masslogmodel_risk)
summary(finallogmodel_risk)

risk_Logpred <- predict(finallogmodel_risk, bmi_risk_data_norm[[2]], type= "response")
risk_Logpred = ifelse(risk_Logpred>.4, 0, 1)

#Safe
masslogmodel_safe <- glm(stroke ~., data = bmi_safe_data_norm[[1]], family = "binomial")
summary(masslogmodel_safe)

finallogmodel_safe <- step(masslogmodel_safe)
summary(finallogmodel_safe)

safe_Logpred <- predict(finallogmodel_safe, bmi_safe_data_norm[[2]], type= "response")
safe_Logpred = ifelse(safe_Logpred>.0004, 0, 1)

#Combine Predictions (The thresholds above gave us the highest kappa statistic)
bmi_combined_Logpred = c(risk_Logpred, safe_Logpred)

bmi_true_label = c(bmi_risk_data_norm[[2]]$stroke, bmi_safe_data_norm[[2]]$stroke)

confusionMatrix(as.factor(bmi_combined_Logpred), as.factor(bmi_true_label))
```

When we split based on BMI, we see even worse results with the model. The accuracy drops all the way down to .6969 while the kappa statistic plummets to .0722. We do see a similar idea where there are different significant variables when predicting "stroke" between the "safe BMI" and "risky BMI" group. However, the confusion matrix output leads us to believe that predicting stroke based on different BMI categories is less effective than age or a general model. The thresholds for the BMI splits were created in similar fashion to the age splits where they were set to optimize kappa but ultimately led to predicting all "stroke" or "no stroke" depending on the BMI category. 

## ANN Model
```{r cache = TRUE}

ANNmodel <- neuralnet(general_norm[[1]]$stroke ~ ., data=general_norm[[1]], hidden = 2, stepmax = 1000000)
plot(ANNmodel, rep = "best")

# obtain model results
ann_results <- compute(ANNmodel, general_norm[[2]])
ann_predicted_stroke <- ann_results$net.result
cutoff = quantile(ann_predicted_stroke, 1-mean(general_norm[[1]]$stroke))
ann_predicted_stroke <- ifelse(ann_predicted_stroke > cutoff, 1, 0)

confusionMatrix(as.factor(ann_predicted_stroke), as.factor(general_norm[[2]]$stroke))
```

Our general ANN model does a poor job in predicting stroke for patients. It has a kappa value of 0.1466, which is comparable to the other individual models.The accuracy of the model is 91.93% overall, and out of 66 patients who do have stroke, our model predicted only 12 of them correctly. This is worrying, especially with so many false negatives. 
Next we want to see if splitting the data based on age and BMI would increase the model's accuracy or kappa statistic for our ANN model.

### ANN Age Based Model
```{r cache = TRUE}
#Young
ANNyoungmodel <- neuralnet(age_young_data_norm[[1]]$stroke ~ ., data=age_young_data_norm[[1]], hidden = 2)
plot(ANNyoungmodel, rep = "best")

# obtain model results
annageyoung_results <- compute(ANNyoungmodel, age_young_data_norm[[2]])
predicted_strokeyoung <- annageyoung_results$net.result
cutoff = quantile(predicted_strokeyoung, 1-mean(age_young_data_norm[[1]]$stroke))
predicted_strokeyoung <- ifelse(predicted_strokeyoung > cutoff, 1, 0)

# Accuracy, Kappa, etc.
confusionMatrix(as.factor(predicted_strokeyoung), as.factor(age_young_data_norm[[2]]$stroke))


#Old
ANNoldmodel <- neuralnet(age_old_data_norm[[1]]$stroke ~ ., data=age_old_data_norm[[1]],hidden = 2)
plot(ANNoldmodel, rep = "best")

# obtain model results
annageold_results <- compute(ANNoldmodel, age_old_data_norm[[2]])
predicted_strokeold <- annageold_results$net.result
cutoff = quantile(predicted_strokeold, 1-mean(age_old_data_norm[[1]]$stroke))
predicted_strokeold <- ifelse(predicted_strokeold > cutoff, 1, 0)

# Accuracy, Kappa, etc.
confusionMatrix(as.factor(predicted_strokeold), as.factor(age_old_data_norm[[2]]$stroke))

#Combined Model
combined_predAge = c(predicted_strokeold, predicted_strokeyoung)
true_label = c(age_old_data_norm[[2]]$stroke, age_young_data_norm[[2]]$stroke)
confusionMatrix(as.factor(combined_predAge), as.factor(true_label))
```

While the overall accuracy (91.39%) took a slight dip, splitting by Age worsened the ANN model marginally. Now, the Kappa statistic has increased to 0.1086.
Now we will see if splitting based on BMI makes any difference in these results.

### ANN BMI Based Model
```{r cache = TRUE}
#Risk
ANNriskmodel <- neuralnet(bmi_risk_data_norm[[1]]$stroke ~ ., data=bmi_risk_data_norm[[1]],hidden =2)
plot(ANNriskmodel, rep = "best")

# obtain model results
annbmirisk_results <- compute(ANNriskmodel, bmi_risk_data_norm[[2]])
predicted_strokerisk <- annbmirisk_results$net.result
cutoff = quantile(predicted_strokerisk, 1-mean(bmi_risk_data_norm[[1]]$stroke))
predicted_strokerisk <- ifelse(predicted_strokerisk > cutoff, 1, 0)

# Accuracy, Kappa, etc.
confusionMatrix(as.factor(predicted_strokerisk), as.factor(bmi_risk_data_norm[[2]]$stroke))


#Safe
ANNsafemodel <- neuralnet(bmi_safe_data_norm[[1]]$stroke ~ ., data=bmi_safe_data_norm[[1]],hidden =2)
plot(ANNsafemodel, rep = "best")

# obtain model results
annbmisafe_results <- compute(ANNsafemodel, bmi_safe_data_norm[[2]])
predicted_strokesafe <- annbmisafe_results$net.result
cutoff = quantile(predicted_strokesafe, 1-mean(bmi_safe_data_norm[[1]]$stroke))
predicted_strokesafe <- ifelse(predicted_strokesafe > cutoff, 1, 0)

# Accuracy, Kappa, etc.
confusionMatrix(as.factor(predicted_strokesafe), as.factor(bmi_safe_data_norm[[2]]$stroke))

#Combined Model
combined_predbmi= c(predicted_strokerisk, predicted_strokesafe)
true_label = c(bmi_risk_data_norm[[2]]$stroke, bmi_safe_data_norm[[2]]$stroke)
confusionMatrix(as.factor(combined_predbmi), as.factor(true_label))
```

Similar to the previous Age-based model, the BMI-based ANN Model slightly increases the accuracy (now 92.72%), although this time the kappa statistic improves to 0.2294, the highest of all the models so far. These figures still are not promising, given that only 17 strokes were accurately predicted.

Overall, it seems like the ANN model did have the best kappa statistic when splitting by BMI. However, the gain in kappa statistic doesn't neccessarily translate to more lives saved, since the false negative rate is still quite high.

## Stacked Model
Next, we wanted to run a stacked model in order to see if a model that combines the findings of each individual model can more accurately predict stroke. 
```{r}
stackedmodel = data.frame(decision_tree = stroke_dt_pred, ann = ann_predicted_stroke, knn = stroke_knn_test_pred, logistic_reg = LogReg_Pred, svm = svm_pred, Stroke = general_norm[[2]]$stroke)

str(svm_pred)

set.seed(42)

testrows <- sample(1:nrow(stackedmodel),950)
stackedtrain <- stackedmodel[-testrows, ]
stackedtest <- stackedmodel[testrows, ]

stackedmodel$decision_tree <- as.numeric(stackedmodel$decision_tree)
stackedmodel$knn <- as.numeric(stackedmodel$knn)

str(stackedmodel)

# Decision Tree based on Train and Test Split
#build model

stacked_dt <- C5.0(as.factor(Stroke) ~ ., data = stackedtrain, costs = error_cost)

plot(stacked_dt)
summary(stacked_dt)

#predict the test data
stacked_dt_pred <- predict(stacked_dt, stackedtest)

#Evaluate prediction results
confusionMatrix(stacked_dt_pred, as.factor(stackedtest$Stroke))
```
Based on the stacked model, it seems that the model is mainly making its decisions off of the decision tree and logistic regression model, but mostly the decision tree model. However, even with all the models' predictions running as variables, the kappa and accuracy statistic of the stacked model does not out perform that of the ANN model individually. The kappa statistic of the stacked model is comparable to the individual performance of the decision tree model or SVM model. 

### Stacked Model Age Based Model
```{r}
stackedmodelage = data.frame(decision_tree = combinedagepredict, ann = combined_predAge, knn = knn_age_combined_pred, logistic_reg = age_combined_Logpred, svm = combined_svmage_pred, Stroke = true_label_age)

str(stackedmodelage)

set.seed(42)

agetestrows <- sample(1:nrow(stackedmodelage),950)
stackedagetrain <- stackedmodelage[-testrows, ]
stackedagetest <- stackedmodelage[testrows, ]

stackedmodelage$decision_tree <- as.numeric(stackedmodelage$decision_tree)
stackedmodelage$knn <- as.numeric(stackedmodelage$knn)

str(stackedmodelage)

# Decision Tree based on Train and Test Split
#build model

stacked_age_dt <- C5.0(as.factor(Stroke) ~ ., data = stackedagetrain, costs = error_cost)

plot(stacked_age_dt)
summary(stacked_age_dt)

#predict the test data
stacked_age_dt_pred <- predict(stacked_age_dt, stackedagetest)

#Evaluate prediction results
confusionMatrix(stacked_age_dt_pred, as.factor(stackedagetest$Stroke))
```
We see that the stacked model does not perform well when we split on age. It has an accuracy and kappa of 0.7126 and 0.1537 respectively, which has lower accuracy and comparable kappa statistic to our other models. This is probably due to the model overly predicting "stroke". Our model is probably doing this because of the error cost for false negatives being quite high, yet it is similar to the cost of our original model. The stacked model is probably not optimal for categorized splits. We also see that there is a high no information rate from the split model, so there is something behind the scenes going on with the models when stacking. It could be because our age split models did not perform well in general. 

### Stacked Model BMI Based Model
```{r}
stackedmodelbmi = data.frame(decision_tree = combinedbmipredict, ann = combined_predbmi, knn = knn_bmi_combined_pred, logistic_reg = bmi_combined_Logpred, svm = combined_svmbmipred, Stroke = true_label_bmi)

str(stackedmodelbmi)

set.seed(42)

bmitestrows <- sample(1:nrow(stackedmodelbmi),950)
stackedbmitrain <- stackedmodelbmi[-testrows, ]
stackedbmitest <- stackedmodelbmi[testrows, ]

stackedmodelbmi$decision_tree <- as.numeric(stackedmodelbmi$decision_tree)
stackedmodelbmi$knn <- as.numeric(stackedmodelbmi$knn)

str(stackedmodelbmi)

# Decision Tree based on Train and Test Split
#build model

stacked_bmi_dt <- C5.0(as.factor(Stroke) ~ ., data = stackedbmitrain, costs = error_cost)

plot(stacked_bmi_dt)
summary(stacked_bmi_dt)

#predict the test data
stacked_bmi_dt_pred <- predict(stacked_bmi_dt, stackedbmitest)

#Evaluate prediction results
confusionMatrix(stacked_bmi_dt_pred, as.factor(stackedbmitest$Stroke))
```
We see a similar result from the BMI stacked model. The kappa statistic is quite low at 0.0853, despite using many more of the individual models to get the final prediction. This is probably due to similar reasons outlined above from the age stacked model. 

## Improved Stacked Model
Next, due to the somewhat light predictability of the stacked model, we wanted to see if we could improve it. 
```{r, warning=FALSE}
set.seed(42)

#Adding Customization Packages
ctrl <- trainControl(method = "cv", number = 10,
                     selectionFunction = "oneSE")

grid <- expand.grid(.model = "tree",
                    .trials = c(1,5,10,15,20,30,35),
                    .winnow = "FALSE")

#Tune the model
TunedStack <- train(as.factor(Stroke) ~ ., data = stackedmodel, method = "C5.0",
           metric = "Kappa",
           trControl = ctrl,
           tuneGrid = grid
          )
TunedStack

#Preparing data for predict and confusion matrix
stackedtestmm <- as.data.frame(model.matrix(~.-1,stackedtest))
stackedtest_norm <- as.data.frame(lapply(stackedtestmm, normalize))
stackedtest_norm$decision_tree0 <- NULL
names(stackedtest_norm)[names(stackedtest_norm) == "decision_tree1"] <- "decision_tree"
names(stackedtest_norm)[names(stackedtest_norm) == "knn1"] <- "knn"

#Predict and Confusion Matrix
tunedstack_pred <- predict(TunedStack, stackedtest_norm)
confusionMatrix(tunedstack_pred, as.factor(stackedtest_norm$Stroke))
```
We see that even with improving the model, the same issue persists where the model is predicting all of one guess. The only difference this time, is that it is guess all "no-stroke". This change could be due to the fact that we do not have the error costs included in the creation of the model, therefore the model predicting all "no-stroke" due to the lack of "stroke" in the data set in general.

### Improved Stacked Model Age Based Model
```{r, warning=FALSE}
set.seed(42)
TunedStackAge <- train(as.factor(Stroke) ~ ., data = stackedmodelage, method = "C5.0",
           metric = "Kappa",
           trControl = ctrl,
           tuneGrid = grid)
TunedStackAge

#Predict and Confusion Matrix
tunedstackAGE_pred <- predict(TunedStackAge, stackedtest_norm)
confusionMatrix(tunedstackAGE_pred, as.factor(stackedtest_norm$Stroke))
```
We see that the improved age model has similar results to the general improved stacked model.

### Improved Stacked Model BMI Based Model
```{r, warning=FALSE}
set.seed(42)
TunedStackBMI <- train(as.factor(Stroke) ~ ., data = stackedmodelbmi, method = "C5.0",
           metric = "Kappa",
           trControl = ctrl,
           tuneGrid = grid)
TunedStackBMI

#Predict and Confusion Matrix
tunedstackBMI_pred <- predict(TunedStackBMI, stackedtest_norm)
confusionMatrix(tunedstackBMI_pred, as.factor(stackedtest_norm$Stroke))
```
Again, we see a similar output. In all of the trials using train, the model only ever predicts 0, both in the training and the test data splits.

## Summary
Overall, this project taught us a lot, but before we get into what it has taught us, let's talk about the insights we could extract from our analysis. In our intro, we talked about how the analysis we get from this project could be applied to predicting if a patient is going to have a stroke or not for both family physicians, hospitals, and insurance companies. We believe that while some of our models are accurate, it is best to use the models as a second opinion. The models are not accurate enough to full rely on them to make decisions. In a world with tons of information flowing and biases influencing decision making choices, a second opinion makes all the difference and can be the difference between life and death. This not only can give more confidence to the first initial prediction (when they match), but it can act as a warning to mismatched predictions, indicating that a deeper dive and more research might be needed in order to make a concrete prediction. Insurance companies and doctors can cross reference between the predictions of the KNN model and the predictions of the decision tree with the prediction of the doctor in order to thoroughly ensure that the right call is being made to the best of their abilities. The KNN model is solely accuracy based, making it a solid second opinion when it comes to deciding whether or not they will have a stroke or not, while the decision tree involves a cost matrix, acting as a solid second opinion consultant for whether to diagnose a patient with risk of stroke or not. 

If we were to run the models again, it would be beneficial to run it on a much larger data set, given that the occurrence of "stroke" is low in general. We would need to increase the data set to the point that it includes many more "stroke" incidents. The KNN model would be the only model that we would feel comfortable using as a heavily weighted second opinion, and even then, it does not include error costs of false negatives, which is something that we want to include because the cost of a false negative is much more severe than the cost of a false positive. The other models are not completely useless though. For example the logistic model, decision tree, and ANN model give us an insight on which variables are more indicative of stroke compared to other variables, a very valuable insight. 

In terms of an AI Canvas, we have a concrete idea of what we are predicting (stroke or not). In terms of judgment, we have an idea of the costs of false negative and false positives, however, more research and an expert opinion from doctors offices and/or hospital rooms would been need to accurately set a realistic error cost. We know that the action would be to diagnose stroke or not and follow-through on the specific procedures that go along with those diagnoses. The outcome would be to follow-up and see if the patient ultimately has a stroke or not. Input involves collect patients age, BMI, average glucose level, gender, etc. Training would involve train and test data being ran through the models above and feedback would involve feeding the outcomes and more collected data back into those models. 

This project taught us a lot of real-life applications to machine learning and predictive analysis. The main lesson we learned was that data input matters. The biggest obstacle we had when running our analysis was the quality and quantity of data within our data set. Our data was very structured and clean. There weren't many missing entries within each observation, however, the number of observations was lacking. While having a smaller number of observations reduces run-time for the models, it causes the results from the models to be less effective in its predictions. For our project, this was the main issue that we had. While there were a good number of observations, it was not nearly enough. There were not enough positive stroke entries to effectivly train the models to accurately predict "stroke." Most of the observations in our data set were "no-stroke". This led to a heavily skewed data set that made it difficult to accurately predict "stroke" due simply to the lack of "strokes" in the data set. Another topic we learned was the importance of setting a cutoff for certain models to predict on. For example, with the logistic regression model and SVM model, it required a cutoff point to predict "stroke" or "no stroke" based on the output probability/log-odds ratio from the model. 

In the end, our model does a really good job at providing more insight as well as a second opinion to predicting strokes. Hosiptals, doctors' practices, and insurance companies can benefit greatly not only from the insights provided by the desciptive models (that give us a more in-depth idea of the variables that significantly effect the probability of strokes) but also provide us with accurate predictions from the KNN model output cross-referenced with the decision tree output. 